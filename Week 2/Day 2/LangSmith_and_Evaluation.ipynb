{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "✋BREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LCEL RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "1f43fdba-f1b4-4cf4-b8b0-3469edc29c80"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using Qdrant, Hugging Face, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and Hugging Face to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs - which will serve as our data for today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `SitemapLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 220/220 [00:35<00:00,  6.13it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4821"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "You are expert assistant able to answer questions based on section \"Context\" and only in that section. Any answers that cannot be found in the section you will reply 'I do not know'.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4o` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "4235f7af-4430-4c08-d94d-99e482ef30af"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What's new in LangChain v0.2?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain v0.2 brings several new features and improvements, including:\\n\\n- A full separation of langchain and langchain-community\\n- New (and versioned) documentation\\n- A more mature and controllable agent framework\\n- Improved LLM interface standardization, particularly around tool calling\\n- Streaming support\\n- 30+ new partner packages\\n\\nThis release is a pre-release, with the full v0.2 release expected in a few weeks.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content='Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. We’re excited to share that v0.2 brings: A much-desired full separation of langchain and langchain-community New (and versioned!) docs A more mature and controllable agent framework Improved LLM interface standardization, particularly around tool callingBetter' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '9e18cea41d96450e82ee2e88b11865b3', '_collection_name': '95a78c562cfb4ae8a6dcb6e13b33e6f0'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.2: A Leap Towards Stability\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "Today, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain.\n",
            "\n",
            "5 min read\n",
            "May 10, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': 'a2431172fcbe4b2992c2e730f8ede2db', '_collection_name': '95a78c562cfb4ae8a6dcb6e13b33e6f0'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.1.0\n",
            "\n",
            "By LangChain\n",
            "10 min read\n",
            "Jan 8, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': 'a96c348c5a70441fb071a09cdf4fc500', '_collection_name': '95a78c562cfb4ae8a6dcb6e13b33e6f0'}\n",
            "----\n",
            "Context:\n",
            "page_content='streaming support30+ new partner packages.This is just a pre-release, with the full v0.2 release coming in a few weeks. Let’s dive into what langchain v0.2 will include.Embracing stability: The evolution of LangChain architectureOne of the most notable changes in langchain v0.2 is the decoupling of the langchain package from langchain-community. As a result, langchain-community now depends on langchain-core and langchain. This is a continuation of the work we began with langchain v0.1.0 to' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': 'c46048681e09423d87c459c184da8728', '_collection_name': '95a78c562cfb4ae8a6dcb6e13b33e6f0'}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangSmith is a framework built on the shoulders of LangChain, designed to track the development lifecycle of AI-powered products and improve observability.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 954, 'total_tokens': 982}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-f6ad65c4-a72d-4b54-8c6c-bc4b09a65e3b-0', usage_metadata={'input_tokens': 954, 'output_tokens': 28, 'total_tokens': 982})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### 🏗️ Activity #1:\n",
        "![title](langsmith.png)\n",
        "\n",
        "The runs or trace in Langsmith shows all the runnable calls of the chain including retrieval from the vectorstore, LLM call, the parallel execution etc... It gives the basic details of the steps that were done in the chain. In addition we get other useful information as latency, costs and number of tokens. Langsmith is useful for figuring out what is under the hood for the Runnable chain.\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DataRepository' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"DataRepository/langchain_blog_test_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [
        {
          "ename": "LangSmithConflictError",
          "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/utils.py:132\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/client.py:793\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    784\u001b[0m         method,\n\u001b[1;32m    785\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs,\n\u001b[1;32m    792\u001b[0m     )\n\u001b[0;32m--> 793\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/utils.py:134\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[1;32m      5\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangsmith-demo-dataset-aie4-triples-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLangChain Blog Test Questions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m test_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     12\u001b[0m   triplet \u001b[38;5;241m=\u001b[39m triplet[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/client.py:2542\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[0;34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, metadata)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_schema_definition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs_schema\n\u001b[0;32m-> 2542\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2548\u001b[0m ls_utils\u001b[38;5;241m.\u001b[39mraise_for_status_with_text(response)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   2552\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   2553\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   2554\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/client.py:829\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    827\u001b[0m     )\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithConflictError(\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithError(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LangSmith\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m     )\n",
            "\u001b[0;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-aie4-triples-v3\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-b05c1cdb' at:\n",
            "https://smith.langchain.com/o/ec69f9d6-8c26-5c15-9a3b-0e9507df49bc/datasets/81b23219-d3e8-4860-aec5-27f1f002169e/compare?selectedSessions=bb74ce28-bd0b-4899-9fc0-b87eaf63c14c\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 9672fd49-f05e-4298-b4ea-6ffc3ef59077: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9658, Requested 1310. Please try again in 5.808s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9658, Requested 1310. Please try again in 5.808s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cb6c29c6-3984-4911-bf09-7665fa3361d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9973, Requested 370. Please try again in 2.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9973, Requested 370. Please try again in 2.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9672fd49-f05e-4298-b4ea-6ffc3ef59077: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9821, Requested 370. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9821, Requested 370. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3a845e13-41c1-46df-b68d-f2786ca41df8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9952, Requested 398. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9952, Requested 398. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9467383d-620c-4313-8af8-087a54583743: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9937, Requested 399. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9937, Requested 399. Please try again in 2.015s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c89ada23-4c86-42f2-9a31-892246d230b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9844, Requested 1472. Please try again in 7.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9844, Requested 1472. Please try again in 7.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc7e8a01-d674-4033-a8e2-3ede0aebe1bc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9842, Requested 1480. Please try again in 7.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9842, Requested 1480. Please try again in 7.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ee2afec1-6d51-4bd9-95af-5de7ef9ea7e2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9834, Requested 1486. Please try again in 7.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9834, Requested 1486. Please try again in 7.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cb6c29c6-3984-4911-bf09-7665fa3361d1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9833, Requested 505. Please try again in 2.027s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9833, Requested 505. Please try again in 2.027s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:12, 12.45s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 9672fd49-f05e-4298-b4ea-6ffc3ef59077: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9810, Requested 536. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9810, Requested 536. Please try again in 2.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:14,  6.52s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 8b6e19c8-c4ec-4ed4-98a0-40927cb2d674: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9689, Requested 1358. Please try again in 6.282s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9689, Requested 1358. Please try again in 6.282s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c89ada23-4c86-42f2-9a31-892246d230b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9978, Requested 392. Please try again in 2.22s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9978, Requested 392. Please try again in 2.22s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9467383d-620c-4313-8af8-087a54583743: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9937, Requested 428. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9937, Requested 428. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:16,  4.24s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 3a845e13-41c1-46df-b68d-f2786ca41df8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9919, Requested 453. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9919, Requested 453. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:16,  2.61s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fd6b3d8f-86be-467e-9fc8-d6cfe281f592: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9735, Requested 636. Please try again in 2.226s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9735, Requested 636. Please try again in 2.226s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cf97a309-f853-490c-ab9f-d38f9334d096: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9701, Requested 439. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9701, Requested 439. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fd6b3d8f-86be-467e-9fc8-d6cfe281f592: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9974, Requested 343. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9974, Requested 343. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b6e19c8-c4ec-4ed4-98a0-40927cb2d674: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9958, Requested 513. Please try again in 2.826s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9958, Requested 513. Please try again in 2.826s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c89ada23-4c86-42f2-9a31-892246d230b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9889, Requested 429. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9889, Requested 429. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "5it [00:19,  2.78s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 421f51f5-d0d6-4f09-b121-e309a3bbab34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9839, Requested 477. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9839, Requested 477. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 96dc96bc-27ab-42a7-9599-20d1124859e2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9947, Requested 363. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9947, Requested 363. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:21,  2.39s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run cf97a309-f853-490c-ab9f-d38f9334d096: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9843, Requested 475. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9843, Requested 475. Please try again in 1.908s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:21,  1.82s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 421f51f5-d0d6-4f09-b121-e309a3bbab34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9868, Requested 348. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9868, Requested 348. Please try again in 1.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86b1dd3c-e826-4220-8cbe-09ce6ebadd17: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9871, Requested 340. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9871, Requested 340. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:23,  1.65s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run e0fb925e-4478-4f02-b96b-04bb5abf4280: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9834, Requested 389. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9834, Requested 389. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:23,  1.21s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 8b6e19c8-c4ec-4ed4-98a0-40927cb2d674: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9727, Requested 555. Please try again in 1.692s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9727, Requested 555. Please try again in 1.692s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:24,  1.13s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 00880dd6-3fe0-4ff9-bc7e-b34bb4f80db6: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9670, Requested 368. Please try again in 228ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9670, Requested 368. Please try again in 228ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:25,  1.20it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 081c8c3d-f693-4a21-af21-b96e9ca07894: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9771, Requested 566. Please try again in 2.022s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9771, Requested 566. Please try again in 2.022s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run aa96fe3d-745f-447d-9102-098ef9ddc3fa: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9697, Requested 632. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9697, Requested 632. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc7e8a01-d674-4033-a8e2-3ede0aebe1bc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9935, Requested 418. Please try again in 2.118s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9935, Requested 418. Please try again in 2.118s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [00:29,  1.30s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run ee2afec1-6d51-4bd9-95af-5de7ef9ea7e2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9595, Requested 425. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9595, Requested 425. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [00:29,  1.02it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run aa96fe3d-745f-447d-9102-098ef9ddc3fa: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9593, Requested 420. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9593, Requested 420. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b4a8de68-7b38-4bdc-ba4f-5f8a12957e79: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9573, Requested 1221. Please try again in 4.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9573, Requested 1221. Please try again in 4.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 48750d28-1552-4dd7-84ac-026a29dd004a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9859, Requested 1350. Please try again in 7.254s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9859, Requested 1350. Please try again in 7.254s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 115bd7dc-279b-473b-a46c-2cce9998c84c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9757, Requested 1791. Please try again in 9.288s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9757, Requested 1791. Please try again in 9.288s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e3447275-457a-412f-9502-62afe9760e43: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9660, Requested 1140. Please try again in 4.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9660, Requested 1140. Please try again in 4.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e3447275-457a-412f-9502-62afe9760e43: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9914, Requested 455. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9914, Requested 455. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 115bd7dc-279b-473b-a46c-2cce9998c84c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9854, Requested 518. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9854, Requested 518. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b876e57b-5b85-4962-9b80-55169fd005d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9841, Requested 1460. Please try again in 7.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9841, Requested 1460. Please try again in 7.806s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [00:41,  3.18s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run b876e57b-5b85-4962-9b80-55169fd005d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9556, Requested 468. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9556, Requested 468. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e3447275-457a-412f-9502-62afe9760e43: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9466, Requested 558. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9466, Requested 558. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "19it [00:43,  2.60s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run b876e57b-5b85-4962-9b80-55169fd005d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9940, Requested 573. Please try again in 3.078s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1267, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 609, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 606, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/Users/richardlai/miniconda3/envs/AIE4/lib/python3.11/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BDHopp1mem9Bs5Ln0tzDGSR4 on tokens per min (TPM): Limit 10000, Used 9940, Requested 573. Please try again in 3.078s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "23it [00:51,  2.23s/it]\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    retrieval_augmented_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "I was not able to get my rate limit issues fixed but I was able to see some results in Langsmith despite the rate limit errors. \n",
        "\n",
        "Basically there are two evaluators I see. For dopeness I see this result.\n",
        "```\n",
        "{\n",
        "  \"key\": \"dopeness\",\n",
        "  \"reasoning\": \"The criterion for this assessment is \\\"dopeness\\\", which is defined as being cool, awesome, and legit. \\n\\n1. Cool: The answer is straightforward and to the point, which could be considered \\\"cool\\\" in the sense that it doesn't beat around the bush. However, it doesn't have any particularly unique or interesting elements that would make it stand out as \\\"cool\\\" in a more general sense.\\n\\n2. Awesome: The answer is accurate and complete, which could be considered \\\"awesome\\\" in the sense that it fully answers the question. However, it doesn't go above and beyond in any way that would make it \\\"awesome\\\" in a more general sense.\\n\\n3. Legit: The answer is definitely \\\"legit\\\" in the sense that it is a valid and correct response to the question.\\n\\nOverall, while the answer meets some aspects of the \\\"dopeness\\\" criterion, it doesn't fully meet all of them. Therefore, the answer does not meet the criteria.\\n\\nN\",\n",
        "  \"value\": \"N\",\n",
        "  \"score\": 0\n",
        "}\n",
        "```\n",
        "The dopeness test seems difficult to get a good score probably because the it is too subjective and needs detailed and concrete criteria.\n",
        "\n",
        "However, for accuracy this seems to be a better metric as there is a comparison to a reference answer. One is able to get a range of scores up to 10. (see below)\n",
        "\n",
        "```\n",
        "{\n",
        "  \"key\": \"score_string:accuracy\",\n",
        "  \"reasoning\": \"The assistant's response is accurate and aligns with the reference answer. It correctly states that Podium improved their agent F1 response quality and reduced engineering intervention by 90% by using LangSmith for dataset curation and fine-tuning. However, the assistant provides additional details not present in the reference answer, such as the increase in F1 scores from 91.7% to 98.6% and the use of LangSmith traces by the TPS team. Despite these additional details, the core information remains the same. Rating: [[9]]\",\n",
        "  \"score\": 9\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17d4193e78a14050a11c5f8306b3ba0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a077888c84bdfbac393d1ebef9d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99751d16888640078ae4820cacab5760",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "value": ""
          }
        },
        "41e287b8dc674f839b6485c9606207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501c5e204387460e8e9a585c9b2ca834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_323a077888c84bdfbac393d1ebef9d6c",
              "IPY_MODEL_710fe93f3241401bafa10a1e0a1bda02",
              "IPY_MODEL_c65a6921701742da9b67c591b19b6336"
            ],
            "layout": "IPY_MODEL_17d4193e78a14050a11c5f8306b3ba0b"
          }
        },
        "54e7ccad2e774261a3313316a1397f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "710fe93f3241401bafa10a1e0a1bda02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e7ccad2e774261a3313316a1397f66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e287b8dc674f839b6485c9606207d5",
            "value": 1
          }
        },
        "99751d16888640078ae4820cacab5760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f5f4beaa8f4a3b94002c5ed122c2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af61b863014d4a8c960570de31a02b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a6921701742da9b67c591b19b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7457f38948d472da4027ba1566b47e4",
            "placeholder": "​",
            "style": "IPY_MODEL_af61b863014d4a8c960570de31a02b3f",
            "value": " 23/? [02:10&lt;00:00,  5.36s/it]"
          }
        },
        "f7457f38948d472da4027ba1566b47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
