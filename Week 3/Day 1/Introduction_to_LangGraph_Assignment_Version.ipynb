{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- ðŸ¤ Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "  - ðŸ¤ Breakout Room #2:\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQ3nRAgoF67"
   },
   "source": [
    "# ðŸ¤ Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n",
    "\n",
    "We'll first install all our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KaVwN269EttM"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set both our OpenAI API key and our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "####ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tool_belt = [\n",
    "    DuckDuckGoSearchRun(),\n",
    "    ArxivQueryRun()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "model = model.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERzuGo6W18Lr"
   },
   "source": [
    "#### â“ Question #1:\n",
    "\n",
    "How does the model determine which tool to use?\n",
    "\n",
    "Firstly the model has to access the situation whether a tool call is necessary. For example if it needs more information based on the query it will choose the appropriate tool to get that information. It knows what tools are available for it to use and the corresponding arguments to correctly call them. The prompt also has instructions on how/when to use the tool in certain situations. The tool itself also has a corresponding description that explain the usage of the tool. The model analyzes the given task or query to identify what kind of operation or information is needed to fulfill the request. Also the model compares the requirements of the task with the capabilities of the available tools. It looks for alignment between what the task needs and what each tool can provide. It can also priotorize which tool to use based on the message history (if any), the relevance, efficiency, and the specificity of the tool to the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `tool_node` is a node which can call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_vF4_lgtmQNo"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YGCbaYqRnmiw"
   },
   "outputs": [],
   "source": [
    "uncompiled_graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1BZgb81VQf9o"
   },
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  return END\n",
    "\n",
    "uncompiled_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UvcgbHf1rIXZ"
   },
   "outputs": [],
   "source": [
    "uncompiled_graph.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "compiled_graph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhNWIwBL1W4Q"
   },
   "source": [
    "#### â“ Question #2:\n",
    "\n",
    "Is there any specific limit to how many times we can cycle?\n",
    "\n",
    "There are no limits to the number of cycles that can occur.\n",
    "\n",
    "If not, how could we impose a limit to the number of cycles?\n",
    "\n",
    "One possible way to limit is user a counter variable perhaps in the state object to make sure that the number cycles do not exceed a certain number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jqfZ0viAAGTkUonYKQPvhBpU', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-33e0c8b4-0ec0-4df6-90bd-f6b61028e93e-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_jqfZ0viAAGTkUonYKQPvhBpU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "[ToolMessage(content='September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ... - Sep 12, 2023 After a season without a captain, the Winnipeg Jets have named their new leader. Centre Adam Lowry, who is about to enter his 10th season as a Jet, was given the honour Tuesday. The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. When the Jets drafted Lowry in 2011\\'s second round, the team\\'s scouts raved about him as captain material.', name='duckduckgo_search', tool_call_id='call_jqfZ0viAAGTkUonYKQPvhBpU')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry. He was named the captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 422, 'total_tokens': 449}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-52072380-0a28-4117-a4bc-b8b3f4bd7b32-0', usage_metadata={'input_tokens': 422, 'output_tokens': 27, 'total_tokens': 449})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
    "\n",
    "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AGlFwDID6JmxMUgvZCNYbasE', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_0fZcF8samMCsj6lfNVWDyfbs', 'function': {'arguments': '{\"query\": \"latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f89c5cf9-6df6-403a-a999-72e5d7d94089-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_AGlFwDID6JmxMUgvZCNYbasE', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_0fZcF8samMCsj6lfNVWDyfbs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: arxiv\n",
      "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_AGlFwDID6JmxMUgvZCNYbasE'), ToolMessage(content='Vice President Kamala Harris condemned former President Donald Trump and his campaign for their actions on Monday at the Arlington National Cemetery in a new post on X. Harris accused Trump of ... About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy & Safety How YouTube works Test new features NFL Sunday Ticket Press Copyright ... Twitter has added a warning to one of President Donald Trump\\'s tweets about protests in Minneapolis. The company says the tweet violated the platform\\'s rules about glorifying violence. U.S ... Twitter. The latest Twitter news and updates. Twitter is a social networking service, primarily microblogging but also a picture and video sharing service, founded by Jack Dorsey, Noah Glass, Biz ... Musk hinted at Twitter\\'s financial troubles saying, \"I acquired the world\\'s largest non-profit for $44 billion.\" (Twitter had, in fact, reported a profit in 2018 and 2019, prior to Musk ...', name='duckduckgo_search', tool_call_id='call_0fZcF8samMCsj6lfNVWDyfbs')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vmK7aqnZ7PIdrkaq5AZ1AvN3', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_L2EZ3oq412v15IgoJgFF2HPq', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_YW0BwWtc33MZJCY8gBsfyl5q', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_lG8Qb6L5kIhlCxvvNWk42FS1', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1385, 'total_tokens': 1493}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d763f43-ce2a-4b7a-96f9-b9af833dd190-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_vmK7aqnZ7PIdrkaq5AZ1AvN3', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_L2EZ3oq412v15IgoJgFF2HPq', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_YW0BwWtc33MZJCY8gBsfyl5q', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_lG8Qb6L5kIhlCxvvNWk42FS1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1385, 'output_tokens': 108, 'total_tokens': 1493})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: duckduckgo_search\n",
      "[ToolMessage(content=\"â€” Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required â€” from over 780GB to less than 48GB â€” to finetune a 65B parameter model. But Hixson's ruling said Meta researcher Tim Dettmers, who was talking with other researchers from the nonprofit group EleutherAI on the instant messaging platform Discord, didn't have the authority to waive attorney-client privilege. Ann Coulter is facing the wrath of social media after posting â€” then deleting â€” a tweet about Tim Walz's son, Gus. Following the third night of the 2024 Democratic National Convention, the ... If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.â€¦\", name='duckduckgo_search', tool_call_id='call_vmK7aqnZ7PIdrkaq5AZ1AvN3'), ToolMessage(content='In this paper, we introduce LLaVA-Phi, a compact vision-language assistant powered by a small language model. Our work combines the powerful open-sourced multi-modal model, LLaVA-1.5 [24], with the best-performing open-sourced small language models, Phi-2 [21].We follow a two-stage training pipeline and leverage high-quality visual instruction tuning data from LLaVA. Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various ... In this paper, we address these aforementioned challenges associated with financial data and introduce FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs). Adopting a data-centric approach, FinGPT underscores the crucial role of data acquisition, cleaning, and preprocessing in developing open-source FinLLMs. We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ... Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. Elo & Sloan (1978) Arpad E Elo and Sam Sloan. The rating of chessplayers: Past and present. Arco Pub., 1978. ISBN 0668047216 9780668047210.', name='duckduckgo_search', tool_call_id='call_L2EZ3oq412v15IgoJgFF2HPq'), ToolMessage(content='The heated on-air dispute last night between MSNBC\\'s Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn\\'t end with Wednesday\\'s segment: Today, Lewandowski tweeted a video in ... MSNBC host Ari Melber, during an interview with Trump campaign adviser Corey Lewandowski on Wednesday, threatened him with a defamation lawsuit for quoting the anchor calling the former President ... Ari Melber warns Trump campaign advisor Corey Lewandowski he \\'will be potentially in a defamation situation\\' on Wednesday\\'s episode of The Beat (MSNBC) \"I did not say that. That is a false ... The team behind QLoRA includes Allen School Ph.D. student Artidoro Pagnoni; alum Ari Holtzman (Ph.D., \\'23), incoming professor at the University of Chicago; and professor Luke Zettlemoyer, who is also a research manager at Meta. Madrona Prize First Runner Up / Punica: Multi-Tenant LoRA Fine-tuned LLM Serving Former Los Angeles Chargers center Will Clapp has been cut by AFC powerhouse, the Buffalo Bills. Ari Meirov of the 33rd Team reported on the events via Twitter/X.', name='duckduckgo_search', tool_call_id='call_YW0BwWtc33MZJCY8gBsfyl5q'), ToolMessage(content=\"Error: RatelimitException('https://duckduckgo.com 202 Ratelimit')\\n Please fix your mistakes.\", name='duckduckgo_search', tool_call_id='call_lG8Qb6L5kIhlCxvvNWk42FS1')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Here are the latest updates I found for the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - Tim Dettmers is joining Ai2 as an AI researcher. He specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization.\\n   - He accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required.\\n\\n2. **Artidoro Pagnoni**:\\n   - No specific latest tweet found, but he is mentioned in the context of QLoRA and its achievements.\\n\\n3. **Ari Holtzman**:\\n   - No specific latest tweet found, but he is mentioned in the context of QLoRA and its achievements.\\n\\n4. **Luke Zettlemoyer**:\\n   - There was an error in retrieving the latest tweet for Luke Zettlemoyer due to a rate limit issue on DuckDuckGo.\\n\\nIf you need more detailed information or specific tweets, you might want to check their Twitter profiles directly.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 223, 'prompt_tokens': 2440, 'total_tokens': 2663}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-d76a950c-c58e-4b1e-a2d6-96ce07f37acf-0', usage_metadata={'input_tokens': 2440, 'output_tokens': 223, 'total_tokens': 2663})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
    "\n",
    "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzDlZVz1Hnf"
   },
   "source": [
    "####ðŸ—ï¸ Activity #2:\n",
    "\n",
    "Please write out the steps the agent took to arrive at the correct answer.\n",
    "\n",
    "Similar to the one before except and addition call to the tool node.\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint (arxiv) to the state object and passed it along the edge to the agent node. This first action node probably looked for the information on the paper that includes the authors names.\n",
    "5. The agent node added a response, an `AIMessage` to the state object and passed it along the conditional edge. \n",
    "6. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg` and sent the state object to the action node again.\n",
    "7. The action node added another Tool response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node. Now they have the authors names it can query the latest tweets.\n",
    "8. The agent node added a response to the state object and passed it along the conditional edge.\n",
    "9. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7c8-Uyarh1v"
   },
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3XeFOT1Sar"
   },
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruQCuzewUuO"
   },
   "source": [
    "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oeXdQgbxwhTv"
   },
   "outputs": [],
   "source": [
    "def convert_inputs(input_object):\n",
    "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
    "\n",
    "def parse_output(input_state):\n",
    "  return input_state[\"messages\"][-1].content\n",
    "\n",
    "agent_chain = convert_inputs | compiled_graph | parse_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9UkCIqkpyZu"
   },
   "source": [
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
    "\n",
    "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    \"What optimizer is used in QLoRA?\",\n",
    "    \"What data type was created in the QLoRA paper?\",\n",
    "    \"What is a Retrieval Augmented Generation system?\",\n",
    "    \"Who authored the QLoRA paper?\",\n",
    "    \"What is the most popular deep learning framework?\",\n",
    "    \"What significant improvements does the LoRA system make?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
    "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
    "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
    "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
    "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
    "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "orYxBZXSxJjZ",
    "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model alone might not have enough information to produce a high-quality response, such as in open-domain question answering, dialogue systems, and other applications requiring access to a large knowledge base.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMXF2KAsQxs"
   },
   "source": [
    "####ðŸ—ï¸ Activity #3:\n",
    "\n",
    "Please create a dataset in the above format with at least 5 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CbagRuJop83E"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What qualitative analysis was described in the QLorA paper that mentions the model's weakness?\",\n",
    "    \"Who are the other authors in the QLorA paper besides Tim Dettmers?\",\n",
    "    \"What is LoRA and how are is it different from QLoRA?\",\n",
    "    \"How does QLoRA achieve memory efficiency?\",\n",
    "    \"What benchmarks or tasks were used to evaluate QLoRA's performance in the QLorA paper?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    {\"must_mention\" : [\"mathematics\", \"mind\"]},\n",
    "    {\"must_mention\" : [\"Zettlemoyer\", \"Holtzman\",\"Pagnoni\"]},\n",
    "    {\"must_mention\" : [\"8-bit\", \"quantization\"]},\n",
    "    {\"must_mention\" : [\"reduced\", \"memory\"]},\n",
    "    {\"must_mention\" : [\"Vicuna\", \"ChatGPT\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7QVFuAmsh7L"
   },
   "source": [
    "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RLfrZrgSsn85"
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\" : q} for q in questions],\n",
    "    outputs=answers,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciV73F9Q04w0"
   },
   "source": [
    "#### â“ Question #3:\n",
    "\n",
    "How are the correct answers associated with the questions?\n",
    "\n",
    "For each question there has to be the corresponding answer with the same index in the list. So in other words it has to be one to one in positioning. If this is not set up correctly then the evaluations will surely be as the expected the answers would not be correct if the positions of the answers were misplaced.\n",
    "\n",
    "> NOTE: Feel free to indicate if this is problematic or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRTXUrTtP9Y"
   },
   "source": [
    "### Task 2: Adding Evaluators\n",
    "\n",
    "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
    "\n",
    "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QrAUXMFftlAY"
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def must_mention(run, example) -> EvaluationResult:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"must_mention\") or []\n",
    "    score = all(phrase in prediction for phrase in required)\n",
    "    return EvaluationResult(key=\"must_mention\", score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNtHORUh0jZY"
   },
   "source": [
    "#### â“ Question #4:\n",
    "\n",
    "What are some ways you could improve this metric as-is?\n",
    "\n",
    "One of the issues is that it seems to be a text match, a rule based approach. There might be cases where there are spelling variations, capitalization vs lowercase, or use of words that equivalent in meaning resulting in false matching. Perhaps a LLM can be used in this function to say that they are equivalent to overcome the rule based approach.\n",
    "> NOTE: Alternatively you can suggest where gaps exist in this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ4DVSXl0BX5"
   },
   "source": [
    "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sL4-XcjytWsu"
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[must_mention],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJr349zhv7"
   },
   "source": [
    "Task 3: Evaluating\n",
    "\n",
    "All that is left to do is evaluate our agent's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5TeCUUkuGld",
    "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'RAG Pipeline - Evaluation - b379a88a' at:\n",
      "https://smith.langchain.com/o/ec69f9d6-8c26-5c15-9a3b-0e9507df49bc/datasets/ffd898b3-534d-4751-94fb-023de73a227a/compare?selectedSessions=f0d9f425-6980-476b-af40-828410886b9b\n",
      "\n",
      "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - e0ea50c4 at:\n",
      "https://smith.langchain.com/o/ec69f9d6-8c26-5c15-9a3b-0e9507df49bc/datasets/ffd898b3-534d-4751-94fb-023de73a227a\n",
      "[------------------------------------------------->] 5/5"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.must_mention</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d1b9122d-44fa-421e-b0c4-eb76b7567a1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.565647</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.739618</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.189285</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.698941</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.441788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.140994</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.357225</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feedback.must_mention error  execution_time  \\\n",
       "count                      5     0        5.000000   \n",
       "unique                     2     0             NaN   \n",
       "top                     True   NaN             NaN   \n",
       "freq                       4   NaN             NaN   \n",
       "mean                     NaN   NaN        5.565647   \n",
       "std                      NaN   NaN        1.739618   \n",
       "min                      NaN   NaN        3.189285   \n",
       "25%                      NaN   NaN        4.698941   \n",
       "50%                      NaN   NaN        5.441788   \n",
       "75%                      NaN   NaN        7.140994   \n",
       "max                      NaN   NaN        7.357225   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      5  \n",
       "unique                                     5  \n",
       "top     d1b9122d-44fa-421e-b0c4-eb76b7567a1a  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'RAG Pipeline - Evaluation - b379a88a',\n",
       " 'results': {'046b016a-647c-4f82-af1e-5edb898ce0e1': {'input': {'question': \"What qualitative analysis was described in the QLorA paper that mentions the model's weakness?\"},\n",
       "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('62a7a126-d044-4e36-b77b-710db5ba4988'), target_run_id=None)],\n",
       "   'execution_time': 7.357225,\n",
       "   'run_id': 'd1b9122d-44fa-421e-b0c4-eb76b7567a1a',\n",
       "   'output': 'The QLoRA paper, titled \"QLoRA: Efficient Finetuning of Quantized LLMs,\" provides a detailed analysis of the model\\'s performance, including its weaknesses. Specifically, the paper mentions a \"lemon-picked analysis\" that demonstrates where the Guanaco model fails compared to ChatGPT. This qualitative analysis highlights the areas where the model underperforms, providing insights into its limitations and weaknesses.',\n",
       "   'reference': {'must_mention': ['mathematics', 'mind']}},\n",
       "  '9f4f7282-c19a-4a50-82a3-61e6caf19042': {'input': {'question': 'Who are the other authors in the QLorA paper besides Tim Dettmers?'},\n",
       "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d822d164-efaf-401d-b258-2228d879989a'), target_run_id=None)],\n",
       "   'execution_time': 3.189285,\n",
       "   'run_id': 'c5acf45d-0a9c-4c53-b86b-cf6012c20123',\n",
       "   'output': 'The authors of the QLoRA paper are:\\n\\n1. Tim Dettmers\\n2. Artidoro Pagnoni\\n3. Ari Holtzman\\n4. Luke Zettlemoyer',\n",
       "   'reference': {'must_mention': ['Zettlemoyer', 'Holtzman', 'Pagnoni']}},\n",
       "  'd4ac8c5b-b6ba-4e4e-9856-59012bbb5237': {'input': {'question': 'What is LoRA and how are is it different from QLoRA?'},\n",
       "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6c4e6e07-d7f5-4bf2-8be9-b1e71861fa49'), target_run_id=None)],\n",
       "   'execution_time': 7.140994,\n",
       "   'run_id': '7d967d2d-6ee5-4f25-93f3-98dde8b7e001',\n",
       "   'output': 'LoRA (Low-Rank Adaptation) and QLoRA (Quantized Low-Rank Adaptation) are techniques used in the field of machine learning, particularly for fine-tuning large language models. Hereâ€™s a brief overview of each and their differences:\\n\\n### LoRA (Low-Rank Adaptation)\\nLoRA is a method designed to fine-tune large pre-trained models efficiently. Instead of updating all the parameters of the model, LoRA introduces a small number of trainable parameters in the form of low-rank matrices. This significantly reduces the computational cost and memory requirements for fine-tuning. The key idea is to decompose the weight updates into low-rank matrices, which can capture the essential information needed for the adaptation without the need to modify the entire model.\\n\\n### QLoRA (Quantized Low-Rank Adaptation)\\nQLoRA builds upon the concept of LoRA by incorporating quantization techniques. Quantization involves reducing the precision of the model parameters (e.g., from 32-bit floating-point to 8-bit integers), which further reduces the memory footprint and computational requirements. QLoRA applies quantization to the low-rank matrices used in LoRA, making the fine-tuning process even more efficient. This allows for the deployment of large models on resource-constrained devices without significant loss in performance.\\n\\n### Key Differences\\n1. **Parameter Efficiency**:\\n   - **LoRA**: Introduces low-rank matrices to reduce the number of trainable parameters.\\n   - **QLoRA**: Further reduces the memory and computational requirements by quantizing the low-rank matrices.\\n\\n2. **Memory and Computational Cost**:\\n   - **LoRA**: Reduces the cost compared to full fine-tuning but still uses floating-point precision.\\n   - **QLoRA**: Further reduces the cost by using lower precision (quantized) parameters.\\n\\n3. **Deployment**:\\n   - **LoRA**: Suitable for scenarios where some computational resources are available but full fine-tuning is too expensive.\\n   - **QLoRA**: Ideal for deployment on resource-constrained devices where memory and computational power are limited.\\n\\nIn summary, while both LoRA and QLoRA aim to make the fine-tuning of large models more efficient, QLoRA takes it a step further by incorporating quantization, making it even more suitable for deployment in resource-limited environments.',\n",
       "   'reference': {'must_mention': ['8-bit', 'quantization']}},\n",
       "  '061b183b-6363-41af-811b-ffed2fdbfb8f': {'input': {'question': 'How does QLoRA achieve memory efficiency?'},\n",
       "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9cbdc52f-4396-4b68-b50a-f352f3200e11'), target_run_id=None)],\n",
       "   'execution_time': 4.698941,\n",
       "   'run_id': '67bb78f7-6280-4e8f-b901-af685ff2f041',\n",
       "   'output': \"QLoRA (Quantized Low-Rank Adaptation) achieves memory efficiency through a combination of quantization and low-rank adaptation techniques. Here's a detailed explanation of how it works:\\n\\n1. **Quantization**:\\n   - **Reduced Precision**: QLoRA uses quantization to reduce the precision of the model parameters. Instead of using 32-bit floating-point numbers (FP32), it uses lower precision formats such as 8-bit integers (INT8). This significantly reduces the memory footprint of the model.\\n   - **Storage Efficiency**: By storing weights in a lower precision format, the amount of memory required to store the model parameters is reduced. This allows larger models to fit into the same amount of memory or enables the same model to run on devices with less memory.\\n\\n2. **Low-Rank Adaptation**:\\n   - **Parameter Efficiency**: Low-rank adaptation involves decomposing the weight matrices into products of smaller matrices. This reduces the number of parameters that need to be stored and updated during training.\\n   - **Efficient Updates**: By focusing on the most significant components of the weight matrices, low-rank adaptation allows for efficient updates and reduces the computational overhead during training.\\n\\n3. **Combination of Techniques**:\\n   - **Synergy**: The combination of quantization and low-rank adaptation allows QLoRA to achieve significant memory savings without compromising the model's performance. Quantization reduces the storage requirements, while low-rank adaptation ensures that the most important information is retained and efficiently updated.\\n\\n4. **Practical Implementation**:\\n   - **Hardware Compatibility**: QLoRA is designed to be compatible with modern hardware accelerators that support lower precision arithmetic, such as GPUs and TPUs. This ensures that the memory and computational benefits of QLoRA can be fully realized in practice.\\n\\nBy leveraging these techniques, QLoRA can train and deploy large-scale models more efficiently, making it possible to use advanced models in resource-constrained environments.\",\n",
       "   'reference': {'must_mention': ['reduced', 'memory']}},\n",
       "  '266dce44-99f4-4cc1-b0b6-58beb7eb1a23': {'input': {'question': \"What benchmarks or tasks were used to evaluate QLoRA's performance in the QLorA paper?\"},\n",
       "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('fd37662c-e63e-429c-aabc-858440e4058d'), target_run_id=None)],\n",
       "   'execution_time': 5.441788,\n",
       "   'run_id': 'de23a37a-3484-4811-a218-8b11348d9832',\n",
       "   'output': 'The QLoRA paper evaluated its performance using a variety of benchmarks and tasks. Here are the key points:\\n\\n1. **Vicuna Benchmark**: QLoRA\\'s best model family, named Guanaco, was evaluated on the Vicuna benchmark, where it outperformed all previously released models and reached 99.3% of the performance level of ChatGPT.\\n\\n2. **Instruction Following and Chatbot Performance**: The paper provides a detailed analysis of instruction following and chatbot performance across 8 instruction datasets. This includes evaluations on multiple model types (e.g., LLaMA, T5) and model scales (e.g., 33B and 65B parameter models).\\n\\n3. **Human and GPT-4 Evaluations**: The performance of the models was assessed based on both human evaluations and GPT-4 evaluations. The paper suggests that GPT-4 evaluations are a cost-effective and reasonable alternative to human evaluation.\\n\\n4. **Chatbot Benchmarks**: The paper also discusses the reliability of current chatbot benchmarks and includes a \"lemon-picked\" analysis to highlight areas where the Guanaco model fails compared to ChatGPT.\\n\\nThe paper emphasizes that QLoRA finetuning on a small high-quality dataset can lead to state-of-the-art results, even with smaller models than the previous state-of-the-art.',\n",
       "   'reference': {'must_mention': ['Vicuna', 'ChatGPT']}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
    "    project_metadata={\"version\": \"1.0.0\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTNe4kWrplB"
   },
   "source": [
    "## Part 2: LangGraph with Helpfulness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1wKRddbIY_S"
   },
   "source": [
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
    "\n",
    "We're going to make a few key adjustments to account for this:\n",
    "\n",
    "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
    "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npTYJ8ayR5B3"
   },
   "source": [
    "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-LQ84YhyJG0w"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD7EV0HqSQcb"
   },
   "source": [
    "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajBwLkFVi1N"
   },
   "source": [
    "####ðŸ—ï¸ Activity #5:\n",
    "\n",
    "Please write markdown for the following cells to explain what each is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6rN7feNVn9f"
   },
   "source": [
    "##### Similar to above below are two nodes, one agent node and one action node. The agent contains the LLM and the action is a tool node to run the tool if there is a tool call. The uncompiled graph or StateGraph called \"graph_with_helpfulness_check\" is created and the add_node functions are called to add to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6r6XXA5FJbVf"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ22o2mWVrfp"
   },
   "source": [
    "##### Below sets the entry point. The starting point will be the agent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HNWHwWxuRiLY"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsXeF6xlaXOZ"
   },
   "source": [
    "##### Below is a conditional router agent. It takes in the state and determines what should the next node to go to either END or to 'continue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "z_Sq3A9SaV1O"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  initial_query = state[\"messages\"][0]\n",
    "  final_response = state[\"messages\"][-1]\n",
    "\n",
    "  if len(state[\"messages\"]) > 10:\n",
    "    return \"END\"\n",
    "\n",
    "  prompt_template = \"\"\"\\\n",
    "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
    "\n",
    "  Initial Query:\n",
    "  {initial_query}\n",
    "\n",
    "  Final Response:\n",
    "  {final_response}\"\"\"\n",
    "\n",
    "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
    "\n",
    "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
    "\n",
    "  if \"Y\" in helpfulness_response:\n",
    "    return \"end\"\n",
    "  else:\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz1u9Vf4SHxJ"
   },
   "source": [
    "####ðŸ—ï¸ Activity #4:\n",
    "\n",
    "Please write what is happening in our `tool_call_or_helpful` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BhnBW2YVsJO"
   },
   "source": [
    "##### In the 'tool_call_or_helpful' function it uses an LLM to determine if the latest response by the tool (last message in the state['messages']) is helpful. It uses the initial query by the user as well as the response by the tool to determine if it is useful. An llm chain exists in the function when invoke will return a 'Y' for yes and 'N' for no. The chain includes a StrOutputParser to ensure the output is a string and as it probably extracts that string from the content variable.\n",
    "\n",
    "##### Below is adding the function to a 'add_conditional_edges' call. This allows after agent to go to three paths: END, 'action' or back to the 'agent' node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aVTKnWMbP_8T"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"action\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGDLEWOIVtK0"
   },
   "source": [
    "##### This creates one edge or path to move from action or tool node to the agent so when the action node completes its process it immediately proceeds to the agent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cbDK2MbuREgU"
   },
   "outputs": [],
   "source": [
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSI8AOaEVvT-"
   },
   "source": [
    "##### This step compiles the state graph to optimize the graph and checks if there are errors in the graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oQldl8ERQ8lf"
   },
   "outputs": [],
   "source": [
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F67FGCMRVwGz"
   },
   "source": [
    "##### Below is the code to run the graph with a user query or input. The call uses astream, an asynchonous stream call to return the response by streaming chunks. The stream_mode as \"updates\". This is the update to the state of the graph after each node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3oo8E-PRK1T",
    "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_OgAEwmxnGWJkwPltJPDQTmTs', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_f3Mvd88pT4Vfa4oM1SENpF7y', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_2E9PdbwERbEWrHYUHXP722Dk', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2adf544b-ea14-4adf-97dd-88eb8a083d58-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_OgAEwmxnGWJkwPltJPDQTmTs', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_f3Mvd88pT4Vfa4oM1SENpF7y', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_2E9PdbwERbEWrHYUHXP722Dk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "[ToolMessage(content='Let\\'s jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA\\'s approach to decomposing ( Î” W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model\\'s learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... Feb 18, 2024. Share. Ever since the introduction of BERT in 2019, fine-tuning has been the standard approach to adapt large language models (LLMs) to downstream tasks. This changed with the introduction of LoRA (Hu et al 2021) which showed for the first time that the weight update matrix during fine-tuning can be drastically simplified using ... \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance.', name='duckduckgo_search', tool_call_id='call_OgAEwmxnGWJkwPltJPDQTmTs'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required â€” from over 780GB to less than 48GB â€” to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... â€” Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.â€¦', name='duckduckgo_search', tool_call_id='call_f3Mvd88pT4Vfa4oM1SENpF7y'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. They all use transformer architecture with attention mechanisms at their core to solve problems across domains. In the Transformer series, we go over the ingredients that have made Transformers a universal recipe for machine learning. First up, we take a visual dive to understand the attention mechanism: Why transformers and attention took over. There are several types of attention mechanisms, each designed to cater to specific use cases. Here are a few notable ones: 1. Self-Attention Mechanism. Self-attention, also known as intra ... The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ...', name='duckduckgo_search', tool_call_id='call_2E9PdbwERbEWrHYUHXP722Dk')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content=\"### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a parameter-efficient fine-tuning method for large language models (LLMs). Traditional fine-tuning involves adjusting the pre-trained weights of a model to adapt it to new tasks, which can be computationally expensive. LoRA simplifies this process by decomposing the weight update matrix into a product of lower-rank matrices. This approach significantly reduces the memory and computational requirements while maintaining the model's performance. LoRA is particularly useful for adapting large pre-trained models to new tasks without the need for extensive computational resources.\\n\\n### Tim Dettmers\\nTim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large pre-trained models. Tim Dettmers is associated with the Allen School and has recently joined the Allen Institute for AI (Ai2) as an AI researcher.\\n\\n### Attention in Machine Learning\\nAttention mechanisms are a fundamental component in modern deep learning models, particularly in natural language processing (NLP) and computer vision. Inspired by the human cognitive process of selective focus, attention mechanisms allow models to focus on relevant parts of the input data, improving performance in tasks such as machine translation, image captioning, and speech recognition.\\n\\nThere are several types of attention mechanisms, including:\\n\\n1. **Self-Attention**: Also known as intra-attention, this mechanism allows a model to focus on different parts of a single sequence. It is a key component of the Transformer architecture, which has revolutionized NLP tasks.\\n\\n2. **Global Attention**: This mechanism considers the entire input sequence when generating each part of the output, making it suitable for tasks like machine translation.\\n\\n3. **Local Attention**: This mechanism focuses on a local subset of the input sequence, which can be more efficient for certain tasks.\\n\\nAttention mechanisms have become a cornerstone in the development of advanced deep learning models, enabling them to handle complex tasks more effectively.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 446, 'prompt_tokens': 1154, 'total_tokens': 1600}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-cdf31364-f7cc-48c7-97fe-865e66c24ddd-0', usage_metadata={'input_tokens': 1154, 'output_tokens': 446, 'total_tokens': 1600})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
    "\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVmZPs6lnpsM"
   },
   "source": [
    "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "Let's ask our system about the 4 patterns of Generative AI:\n",
    "\n",
    "1. Prompt Engineering\n",
    "2. RAG\n",
    "3. Fine-tuning\n",
    "4. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZoLl7GlXoae-"
   },
   "outputs": [],
   "source": [
    "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkh0YJuCp3Zl",
    "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
      "\n",
      "### Key Aspects of Prompt Engineering:\n",
      "1. **Crafting Effective Prompts**: Designing questions or statements that guide the AI to produce the desired type of response.\n",
      "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality and relevance of the output.\n",
      "3. **Understanding Model Behavior**: Gaining insights into how different prompts affect the model's responses, which can involve understanding the model's training data and inherent biases.\n",
      "\n",
      "### Emergence of Prompt Engineering:\n",
      "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to explore and document best practices for interacting with these advanced AI systems.\n",
      "\n",
      "To get more precise information on the timeline and development of prompt engineering, I can look up recent articles or papers on the topic. Would you like me to do that?\n",
      "\n",
      "\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) was first introduced in a 2020 research paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" authored by Patrick Lewis and a team at Facebook AI Research (now Meta). This technique represents a significant advancement in the field of natural language processing by combining the strengths of retrieval-based and generation-based models to enhance the quality and relevance of generated text.\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks. Fine-tuning is particularly popular in natural language processing (NLP) and computer vision.\n",
      "\n",
      "### Key Steps in Fine-Tuning:\n",
      "1. **Pre-training**: A model is initially trained on a large, general dataset.\n",
      "2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset.\n",
      "\n",
      "### Benefits of Fine-Tuning:\n",
      "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
      "- **Performance**: Often results in better performance on the specific task due to the model's prior knowledge.\n",
      "\n",
      "### Historical Context:\n",
      "Fine-tuning became particularly prominent with the advent of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models demonstrated that fine-tuning could achieve state-of-the-art results on a variety of NLP tasks.\n",
      "\n",
      "To find out when fine-tuning specifically broke onto the scene, I can look up relevant information. Would you like me to do that?\n",
      "\n",
      "\n",
      "\n",
      "LLM-based agents, or Large Language Model-based agents, are autonomous systems that leverage large language models to perform tasks, interact with users, and adapt to various inputs and environments. These agents are equipped with capabilities for memory, planning, and execution, allowing them to autonomously perform tasks and respond to user requests.\n",
      "\n",
      "The concept of LLM-based agents has been a focal point of research in both academia and industry, leading to a surge in studies exploring their potential. The introduction of these agents can be traced back to the advancements in large language models and their applications in natural language processing and artificial intelligence. The exact timeline of their \"break onto the scene\" is not pinpointed in the search results, but it is closely tied to the development and deployment of advanced language models like GPT-3 and beyond, which gained significant attention around 2020.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pattern in patterns:\n",
    "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
    "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
    "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "  print(messages[\"messages\"][-1].content)\n",
    "  print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
